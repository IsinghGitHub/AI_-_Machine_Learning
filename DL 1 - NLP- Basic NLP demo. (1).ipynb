{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# CellStrat Hub Pack - Natural Language Processing\n",
    "# Compatible tier : Free Tier or above \n",
    "# Kernel : Pytorch 1.9 \n",
    "#=============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# If any library needs to be installed, install with following command :-\n",
    "# pip install <library-name>\n",
    "# This pip command should be in an independent cell with no other code or comments in this cell.\n",
    "#==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Download the English package by running this command in the terminal:-\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "#==============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora, Tokens, and Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All NLP methods, be they classic or modern, begin with a text dataset, also called a **corpus** (plural: corpora). A corpus usually contains raw text (in ASCII or UTF-8) and any metadata associated with the text. The raw text is a sequence of characters (bytes), but most times it is useful to group those characters into contiguous units called tokens. \n",
    "\n",
    "The process of breaking a text down into tokens is called tokenization. For example, there are six tokens in the Esperanto sentence “**Mary, don’t slap the green witch**.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# Basic concepts of NLP:\n",
    "# Corpora, Tokens, and Types\n",
    "# Text Corpora\n",
    "# Unigrams, Bigrams, Trigrams, …, N-grams\n",
    "# Lemmatization\n",
    "# Stop Words\n",
    "#======================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (8.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ubuntu/anaconda3/envs/pytorch/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#Import the spacy library\n",
    "#==============================================================================\n",
    "\n",
    "import spacy\n",
    "#load the general english library\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees']\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Example of using Tweet tokenizer to tokenize the tweets\n",
    "#==============================================================================\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet=u\"Snow White and the Seven Degrees\"\n",
    "    #MakeAMovieCold@midnight:-)\"\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Download the gutenberg corpus\n",
    "#=============================================================================\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#gutenberg corpus has lot of different text files \n",
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#Point to austen-emm.txt file\n",
    "#==============================================================================\n",
    "\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The austen-emma.tx is having tokenized words\n",
    "emma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the length of emma document ( # of words)\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Toeknized sequences from the shakespeare-macbeth document\n",
    "#==============================================================================\n",
    "from nltk.corpus import gutenberg\n",
    "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "macbeth_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Double',\n",
       " ',',\n",
       " 'double',\n",
       " ',',\n",
       " 'toile',\n",
       " 'and',\n",
       " 'trouble',\n",
       " ';',\n",
       " 'Fire',\n",
       " 'burne',\n",
       " ',',\n",
       " 'and',\n",
       " 'Cauldron',\n",
       " 'bubble']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Print a specific sequence\n",
    "#==============================================================================\n",
    "macbeth_sentences[1116]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora in Other Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/indian.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Download the Indian package\n",
    "#==============================================================================\n",
    "\n",
    "nltk.download('indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['पूर्ण', 'प्रतिबंध', 'हटाओ', ':', 'इराक', 'संयुक्त', ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Display he hindi words\n",
    "#==============================================================================\n",
    "\n",
    "nltk.corpus.indian.words('hindi.pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams, Bigrams, Trigrams, …, N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', ','], [',', \"n't\"], [\"n't\", 'slap'], ['slap', 'green'], ['green', 'witch'], ['witch', '.']]\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Return the unigrams(one word),bigrams(two words),trigrams (three words..)\n",
    "#==============================================================================\n",
    "\n",
    "def n_grams(text, n):\n",
    "    '''\n",
    "    takes tokens or text, returns a list of n-grams\n",
    "    '''\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned = ['mary', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
    "print(n_grams(cleaned, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me', 'et', 'th', 'ha', 'an', 'no', 'ol']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Split a given word into bi grams\n",
    "#==============================================================================\n",
    "\n",
    "name ='methanol'\n",
    "[name[i:i+2] for i in range(len(name)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install ngrams Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ngrams\n",
      "  Downloading ngrams-1.0.3.tar.gz (1.3 kB)\n",
      "Building wheels for collected packages: ngrams\n",
      "  Building wheel for ngrams (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ngrams: filename=ngrams-1.0.3-py3-none-any.whl size=1590 sha256=74ed27cf6fd9ce547118f5f3cf2ad4e76c09cf01ebf8c2616986b7bcdcb50d29\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/54/4e/39/0d67a8d09e359697785cb9cb2ca6c075b4c9671cad607df2df\n",
      "Successfully built ngrams\n",
      "Installing collected packages: ngrams\n",
      "Successfully installed ngrams-1.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['This', 'class', 'is', 'part', 'of', 'a', 'advance', 'NLP', 'course', '@', 'CellStrat', '.']\n",
      "2-gram:  ['This class', 'class is', 'is part', 'part of', 'of a', 'a advance', 'advance NLP', 'NLP course', 'course @', '@ CellStrat', 'CellStrat .']\n",
      "3-gram:  ['This class is', 'class is part', 'is part of', 'part of a', 'of a advance', 'a advance NLP', 'advance NLP course', 'NLP course @', 'course @ CellStrat', '@ CellStrat .']\n",
      "4-gram:  ['This class is part', 'class is part of', 'is part of a', 'part of a advance', 'of a advance NLP', 'a advance NLP course', 'advance NLP course @', 'NLP course @ CellStrat', 'course @ CellStrat .']\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Inbuilt library can be used to produce n-grams\n",
    "#==============================================================================\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "#==============================================================================\n",
    "# Function to generate n-grams from sentences.\n",
    "#==============================================================================\n",
    "\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    " \n",
    "data = 'This class is part of a advance NLP course @CellStrat.'\n",
    " \n",
    "print(\"1-gram: \", extract_ngrams(data, 1))\n",
    "print(\"2-gram: \", extract_ngrams(data, 2))\n",
    "print(\"3-gram: \", extract_ngrams(data, 3))\n",
    "print(\"4-gram: \", extract_ngrams(data, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> he\n",
      "was --> be\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Lemmatization is the process of converting to the root word\n",
    "#==============================================================================\n",
    "doc = nlp(u\"he was running late\")\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, token.lemma_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Use the wordnet package \n",
    "#WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.\n",
    "#==============================================================================\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She gripped the armrest a he passed two car at a time.\n",
      "Her car wa in full view.\n",
      "A number of car carried out of state license plates.\n",
      "**********\n",
      "jump\n",
      "jump\n",
      "jump\n",
      "**********\n",
      "sad\n",
      "happy\n",
      "easy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "wnl = WordNetLemmatizer()\n",
    "text = ['She gripped the armrest as he passed two cars at a time.',\n",
    "        'Her car was in full view.',\n",
    "        'A number of cars carried out of state license plates.']\n",
    " \n",
    "output = []\n",
    "for sentence in text:\n",
    "    output.append(\" \".join([wnl.lemmatize(i) for i in sentence.split()]))\n",
    " \n",
    "for item in output:\n",
    "    print(item)\n",
    " \n",
    "print(\"*\" * 10)\n",
    "print(wnl.lemmatize('jumps', 'n'))\n",
    "print(wnl.lemmatize('jumping', 'v'))\n",
    "print(wnl.lemmatize('jumped', 'v'))\n",
    " \n",
    "print(\"*\" * 10)\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('happiest', 'a'))\n",
    "print(wnl.lemmatize('easiest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'class', 'part', 'advance', 'NLP', 'course', '@', 'CellStrat', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This class is part of a advance NLP course @CellStrat.\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizing Words: POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLTK Tagger\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "#import the libraries\n",
    "#==============================================================================\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input a given text\n",
    "txt ='I saw a girl with a telescope.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('saw', 'VBD'), ('girl', 'JJ'), ('telescope', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "#Toeknize the sentences\n",
    "#==============================================================================\n",
    "tokenized = sent_tokenize(txt) \n",
    "for i in tokenized: \n",
    "      \n",
    "    # Word tokenizers is used to find the words  \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "  \n",
    "    # removing stop words from wordList \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech  \n",
    "    # tagger or POS-tagger.  \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "  \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - NP\n",
      "a girl - NP\n",
      "a telescope - NP\n"
     ]
    }
   ],
   "source": [
    "#==============================================================================\n",
    "\n",
    "#Np stands for Noun Phrase\n",
    "#Display only the noun phrases\n",
    "\n",
    "#==============================================================================\n",
    "doc  = nlp(u\"I saw a girl with a telescope.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print ('{} - {}'.format(chunk, chunk.label_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
